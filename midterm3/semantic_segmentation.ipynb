{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISPR 2025 - Midterm 3 - Assignament 2 - Gianluca Panzani (550358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_dir: str, prefix: str, index: int, postfix: str, mask=False) -> Image:\n",
    "    zeros = '00' if index < 10 else '0' if index < 100 else ''\n",
    "    full_path = f'{img_dir}{prefix}{zeros}{index}{postfix}'\n",
    "    if not mask:\n",
    "        return Image.open(full_path).convert('RGB')\n",
    "    return Image.open(full_path).convert('L')\n",
    "\n",
    "def from_img_to_tensor(img: Image, resize_shape: tuple, mean=None, std=None) -> torch.Tensor:\n",
    "    if mean is not None and std is not None:\n",
    "        if not (len(mean) == 3 and len(std) == 3):\n",
    "            raise ValueError('The image has 3 channles. The parameters \"mean\" and \"std\" have to be of length equal to 3.')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(resize_shape), # Resize\n",
    "            transforms.ToTensor(), # Scaling\n",
    "            transforms.Normalize(mean, std) # Normalization\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(resize_shape), # Resize\n",
    "            transforms.ToTensor() # Scaling\n",
    "        ])\n",
    "    return transform(img)\n",
    "    \n",
    "def from_mask_to_tensor(img: Image, resize_shape: tuple) -> torch.Tensor:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(resize_shape), # Resize\n",
    "        transforms.ToTensor(), # Scaling\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    return (img > 0.5).float() # Binarization\n",
    "\n",
    "def get_mean_and_std(images_dir: str, img_prefix: str, img_postfix: str, resize_shape: tuple):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        try:\n",
    "            image = get_img(img_dir=images_dir, prefix=img_prefix, index=i, postfix=img_postfix, mask=False)\n",
    "        except:\n",
    "            break\n",
    "        img = from_img_to_tensor(img=image, resize_shape=resize_shape)\n",
    "        mean += img.mean(dim=(1, 2)) # Mean computed on each channel (e.g. with RGB -> [meanR,meanG,meanB])\n",
    "        std += img.std(dim=(1, 2)) # Std computed on each channel (e.g. with RGB -> [stdR,stdG,stdB])\n",
    "    mean = (mean / (i-1)).tolist()\n",
    "    std = (std / (i-1)).tolist()\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, img_prefix, img_postfix, masks_dir, mask_prefix, mask_postfix, resize_shape=(128,128)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.resize_shape = resize_shape\n",
    "        self.img_prefix = img_prefix\n",
    "        self.mask_prefix = mask_prefix\n",
    "        self.img_postfix = img_postfix\n",
    "        self.mask_postfix = mask_postfix\n",
    "        self.length = len(os.listdir(images_dir))\n",
    "        mean, std = get_mean_and_std(\n",
    "            images_dir=self.images_dir,\n",
    "            img_prefix=self.img_prefix,\n",
    "            img_postfix=self.img_postfix,\n",
    "            resize_shape=self.resize_shape\n",
    "        )\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = get_img(self.images_dir, self.img_prefix, index+1, self.img_postfix, mask=False)\n",
    "        mask = get_img(self.masks_dir, self.mask_prefix, index+1, self.mask_postfix, mask=True)\n",
    "        img_tensor = from_img_to_tensor(img, self.resize_shape, self.mean, self.std)\n",
    "        mask_tensor = from_mask_to_tensor(mask, self.resize_shape)\n",
    "        return img_tensor, mask_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n",
    "# Creation of the dataset object\n",
    "dataset = HorseSegmentationDataset(\n",
    "    images_dir='weizmann_horse_db/horse/',\n",
    "    img_prefix='horse',\n",
    "    img_postfix='.png',\n",
    "    masks_dir='weizmann_horse_db/mask/',\n",
    "    mask_prefix='horse',\n",
    "    mask_postfix='.png',\n",
    "    resize_shape=(128,128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_channels: list[int],\n",
    "                 decoder_channels: list[int],\n",
    "                 encoder_kernel_sizes: list[int],\n",
    "                 decoder_kernel_sizes: list[int],\n",
    "                 encoder_strides: list[int],\n",
    "                 decoder_strides: list[int],\n",
    "                 output_padding: int):\n",
    "        super(SegmentationCNN, self).__init__()\n",
    "        # Encoder architecture\n",
    "        layers = []\n",
    "        for i in range(len(encoder_channels)-1):\n",
    "            k = encoder_kernel_sizes[i]\n",
    "            s = encoder_strides[i]\n",
    "            layers.append(nn.Conv2d(encoder_channels[i], encoder_channels[i+1], kernel_size=k, stride=s, padding=k//2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        # Decoder architecture\n",
    "        layers = []\n",
    "        for i in range(len(decoder_channels)-2):\n",
    "            k = decoder_kernel_sizes[i]\n",
    "            s = decoder_strides[i]\n",
    "            layers.append(nn.ConvTranspose2d(decoder_channels[i], decoder_channels[i+1], kernel_size=k, stride=s, padding=k, output_padding=output_padding))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.ConvTranspose2d(decoder_channels[-2], decoder_channels[-1], kernel_size=decoder_kernel_sizes[-1], stride=decoder_strides[-1]))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def train_model(model: nn.Module, tr_dataloader: DataLoader, vl_dataloader: DataLoader, optimizer, criterion, device, epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Iterate on the epochs\n",
    "    tr_losses = []\n",
    "    vl_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate on the batches (of the training set)\n",
    "        for imgs, masks in tr_dataloader:\n",
    "            imgs = imgs.to(device)          # Move images' tensors on GPU or CPU\n",
    "            masks = masks.to(device)        # Move masks' tensors on GPU or CPU\n",
    "\n",
    "            preds = model(imgs)             # Compute the predictions\n",
    "            loss = criterion(preds, masks)  # Compute the loss\n",
    "            optimizer.zero_grad()           # Reset the gradients\n",
    "            loss.backward()                 # Perform backpropagation\n",
    "            optimizer.step()                # Update model's parameters (based on gradients)\n",
    "\n",
    "            total_loss += loss.item()       # Update total loss with the average loss on this batch\n",
    "\n",
    "        # Compute the average loss on the epoch\n",
    "        tr_avg_loss = total_loss / len(tr_dataloader)\n",
    "        tr_losses.append(tr_avg_loss)\n",
    "\n",
    "        # Start validation phase\n",
    "        model.eval()\n",
    "\n",
    "        # Disable the update of the gradients\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Iterations on the batches (of the validation set)\n",
    "            for imgs, masks in vl_dataloader:\n",
    "                imgs = imgs.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, masks)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        vl_avg_loss = total_loss / len(vl_dataloader)\n",
    "        vl_losses.append(vl_avg_loss)\n",
    "\n",
    "        # Print of the epoch result\n",
    "        print(f'Epoch {epoch+1}/{epochs}: TR_loss={tr_avg_loss:.4f} - VL_loss={vl_avg_loss:.4f}')\n",
    "    \n",
    "    return tr_losses, vl_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "The best hyperparameters are searched with the Grid search technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_combinations(params: dict) -> list[dict]:\n",
    "    '''\n",
    "    Creates and saves into the class instance a list with all the possible combinations of parameters \\\n",
    "    in the dictionary \"params\".\n",
    "\n",
    "    Parameters:\n",
    "    - params: dictionary with the parameters as associations <key:values> (with: key=parameter_name,\n",
    "    values=possible_values_list).\n",
    "\n",
    "    Returns:\n",
    "    A list of dictionaries such that each one represents a combination of parameters (1 combination = 1 dictionary).\n",
    "    '''\n",
    "    params_index_dict = {}\n",
    "    params_combinations = []\n",
    "    for key in params.keys():\n",
    "        params_index_dict[key] = 0 # current_index for that key\n",
    "    while sum([index+1 for _, index in params_index_dict.items()]) != sum(len(val_list) for _, val_list in params.items()):\n",
    "        params_i = {}\n",
    "        for key, i in params_index_dict.items():\n",
    "            params_i[key] = params[key][i]\n",
    "        params_combinations.append(params_i)\n",
    "        for key in params_index_dict.keys():\n",
    "            params_index_dict[key] += 1\n",
    "            if params_index_dict[key] < len(params[key]):\n",
    "                break\n",
    "            params_index_dict[key] = 0\n",
    "    params_i = {}\n",
    "    for key, i in params_index_dict.items():\n",
    "        params_i[key] = params[key][i]\n",
    "    params_combinations.append(params_i)\n",
    "    return params_combinations\n",
    "\n",
    "\n",
    "# Space of the hyperparameters\n",
    "params_space = {\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'epochs': [10, 30],\n",
    "    'architecture': [\n",
    "        {\n",
    "            'encoder_channels': [3,32,64], 'encoder_kernel_sizes': [3,3,3], 'encoder_strides': [1,1,1],\n",
    "            'decoder_channels': [64,32,1], 'decoder_kernel_sizes': [3,3,3], 'decoder_strides': [1,1,1],\n",
    "            'output_padding': 0\n",
    "        },\n",
    "        {\n",
    "            'encoder_channels': [3,32,64], 'encoder_kernel_sizes': [3,3,3], 'encoder_strides': [2,2,2],\n",
    "            'decoder_channels': [64,32,1], 'decoder_kernel_sizes': [2,2,2], 'decoder_strides': [2,2,2],\n",
    "            'output_padding': 0\n",
    "        },\n",
    "        {\n",
    "            'encoder_channels': [3,32,64], 'encoder_kernel_sizes': [5,5,5], 'encoder_strides': [2,2,2],\n",
    "            'decoder_channels': [64,32,1], 'decoder_kernel_sizes': [4,4,4], 'decoder_strides': [2,2,2],\n",
    "            'output_padding': 0\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Create the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m model \u001b[39m=\u001b[39m SegmentationCNN(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39marchitecture\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCELoss()\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mSegmentationCNN.__init__\u001b[0;34m(self, encoder_channels, decoder_channels, encoder_kernel_sizes, decoder_kernel_sizes, encoder_strides, decoder_strides, output_padding)\u001b[0m\n\u001b[1;32m     27\u001b[0m layers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mConvTranspose2d(decoder_channels[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], decoder_channels[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], kernel_size\u001b[39m=\u001b[39mdecoder_kernel_sizes[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], stride\u001b[39m=\u001b[39mdecoder_strides[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[1;32m     28\u001b[0m layers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mSigmoid())\n\u001b[0;32m---> 29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(layers)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:106\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args):\n\u001b[0;32m--> 106\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_module(\u001b[39mstr\u001b[39m(idx), module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:607\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Add a child module to the current module.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \n\u001b[1;32m    599\u001b[0m \u001b[39mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(module, Module) \u001b[39mand\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mtypename(module)\u001b[39m}\u001b[39;00m\u001b[39m is not a Module subclass\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    609\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule name should be a string. Got \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mtypename(name)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def show_plot(tr_losses: list, vl_losses: list):\n",
    "    plt.plot(tr_losses, label='Training Loss')\n",
    "    plt.plot(vl_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss functions')\n",
    "    plt.show()\n",
    "\n",
    "# Iterates on each parameters combination obtained from the parameters space\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.70,0.15,0.15])\n",
    "for params in get_params_combinations(params_space):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Create the model\n",
    "    model = SegmentationCNN(**params['architecture'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # Training phase\n",
    "    tr_losses, vl_losses = train_model(\n",
    "        model,\n",
    "        tr_dataloader=train_loader,\n",
    "        vl_dataloader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        epochs=params['epochs']\n",
    "    )\n",
    "\n",
    "    # Shows the Loss plots\n",
    "    show_plot(tr_losses, vl_losses)\n",
    "\n",
    "    # Testing phase\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
