{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISPR 2025 - Midterm 3 - Assignament 2 - Gianluca Panzani (550358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_dir: str, prefix: str, index: int, postfix: str, mask=False) -> Image:\n",
    "    zeros = '00' if index < 10 else '0' if index < 100 else ''\n",
    "    full_path = f'{img_dir}{prefix}{zeros}{index}{postfix}'\n",
    "    if not mask:\n",
    "        return Image.open(full_path).convert('RGB')\n",
    "    return Image.open(full_path).convert('L')\n",
    "\n",
    "def from_img_to_tensor(img: Image, resize_shape: tuple, mean=None, std=None) -> torch.Tensor:\n",
    "    if mean is not None and std is not None:\n",
    "        if not (len(mean) == 3 and len(std) == 3):\n",
    "            raise ValueError('The image has 3 channles. The parameters \"mean\" and \"std\" have to be of length equal to 3.')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(resize_shape), # Resize\n",
    "            transforms.ToTensor(), # Scaling\n",
    "            transforms.Normalize(mean, std) # Normalization\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(resize_shape), # Resize\n",
    "            transforms.ToTensor() # Scaling\n",
    "        ])\n",
    "    return transform(img)\n",
    "    \n",
    "def from_mask_to_tensor(img: Image, resize_shape: tuple) -> torch.Tensor:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(resize_shape), # Resize\n",
    "        transforms.ToTensor(), # Scaling\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    return (img > 0.5).float() # Binarization\n",
    "\n",
    "def get_mean_and_std(images_dir: str, img_prefix: str, img_postfix: str, resize_shape: tuple):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        try:\n",
    "            image = get_img(img_dir=images_dir, prefix=img_prefix, index=i, postfix=img_postfix, mask=False)\n",
    "        except:\n",
    "            break\n",
    "        img = from_img_to_tensor(img=image, resize_shape=resize_shape)\n",
    "        mean += img.mean(dim=(1, 2)) # Mean computed on each channel (e.g. with RGB -> [meanR,meanG,meanB])\n",
    "        std += img.std(dim=(1, 2)) # Std computed on each channel (e.g. with RGB -> [stdR,stdG,stdB])\n",
    "    mean = (mean / (i-1)).tolist()\n",
    "    std = (std / (i-1)).tolist()\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, img_prefix, img_postfix, masks_dir, mask_prefix, mask_postfix, resize_shape=(128,128)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.resize_shape = resize_shape\n",
    "        self.img_prefix = img_prefix\n",
    "        self.mask_prefix = mask_prefix\n",
    "        self.img_postfix = img_postfix\n",
    "        self.mask_postfix = mask_postfix\n",
    "        self.length = len(os.listdir(images_dir))\n",
    "        mean, std = get_mean_and_std(\n",
    "            images_dir=self.images_dir,\n",
    "            img_prefix=self.img_prefix,\n",
    "            img_postfix=self.img_postfix,\n",
    "            resize_shape=self.resize_shape\n",
    "        )\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = get_img(self.images_dir, self.img_prefix, index+1, self.img_postfix, mask=False)\n",
    "        mask = get_img(self.masks_dir, self.mask_prefix, index+1, self.mask_postfix, mask=True)\n",
    "        img_tensor = from_img_to_tensor(img, self.resize_shape, self.mean, self.std)\n",
    "        mask_tensor = from_mask_to_tensor(mask, self.resize_shape)\n",
    "        return img_tensor, mask_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n",
    "# Creation of the dataset object\n",
    "dataset = HorseSegmentationDataset(\n",
    "    images_dir='weizmann_horse_db/horse/',\n",
    "    img_prefix='horse',\n",
    "    img_postfix='.png',\n",
    "    masks_dir='weizmann_horse_db/mask/',\n",
    "    mask_prefix='horse',\n",
    "    mask_postfix='.png',\n",
    "    resize_shape=(128,128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_channels: list[int],\n",
    "                 decoder_channels: list[int],\n",
    "                 encoder_kernel_sizes: list[int],\n",
    "                 decoder_kernel_sizes: list[int],\n",
    "                 encoder_strides: list[int],\n",
    "                 decoder_strides: list[int],\n",
    "                 output_padding: int):\n",
    "        super(SegmentationCNN, self).__init__()\n",
    "        # Encoder architecture\n",
    "        layers = []\n",
    "        for i in range(len(encoder_channels)-1):\n",
    "            k = encoder_kernel_sizes[i]\n",
    "            s = encoder_strides[i]\n",
    "            layers.append(nn.Conv2d(encoder_channels[i], encoder_channels[i+1], kernel_size=k, stride=s, padding=k//2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        # Decoder architecture\n",
    "        layers = []\n",
    "        for i in range(len(decoder_channels)-2):\n",
    "            k = decoder_kernel_sizes[i]\n",
    "            s = decoder_strides[i]\n",
    "            layers.append(nn.ConvTranspose2d(decoder_channels[i], decoder_channels[i+1], kernel_size=k, stride=s, padding=k, output_padding=output_padding))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.ConvTranspose2d(decoder_channels[-2], decoder_channels[-1], kernel_size=decoder_kernel_sizes[-1], stride=decoder_strides[-1]))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def train_model(model: nn.Module, tr_dataloader: DataLoader, vl_dataloader: DataLoader, optimizer, criterion, device, epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Iterate on the epochs\n",
    "    tr_losses = []\n",
    "    vl_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate on the batches (of the training set)\n",
    "        for imgs, masks in tr_dataloader:\n",
    "            imgs = imgs.to(device)          # Move images' tensors on GPU or CPU\n",
    "            masks = masks.to(device)        # Move masks' tensors on GPU or CPU\n",
    "\n",
    "            preds = model(imgs)             # Compute the predictions\n",
    "            loss = criterion(preds, masks)  # Compute the loss\n",
    "            optimizer.zero_grad()           # Reset the gradients\n",
    "            loss.backward()                 # Perform backpropagation\n",
    "            optimizer.step()                # Update model's parameters (based on gradients)\n",
    "\n",
    "            total_loss += loss.item()       # Update total loss with the average loss on this batch\n",
    "\n",
    "        # Compute the average loss on the epoch\n",
    "        tr_avg_loss = total_loss / len(tr_dataloader)\n",
    "        tr_losses.append(tr_avg_loss)\n",
    "\n",
    "        # Start validation phase\n",
    "        model.eval()\n",
    "\n",
    "        # Disable the update of the gradients\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Iterations on the batches (of the validation set)\n",
    "            for imgs, masks in vl_dataloader:\n",
    "                imgs = imgs.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, masks)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        vl_avg_loss = total_loss / len(vl_dataloader)\n",
    "        vl_losses.append(vl_avg_loss)\n",
    "\n",
    "        # Print of the epoch result\n",
    "        print(f'Epoch {epoch+1}/{epochs}: TR_loss={tr_avg_loss:.4f} - VL_loss={vl_avg_loss:.4f}')\n",
    "    \n",
    "    return tr_losses, vl_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "The best hyperparameters are searched with the Grid search technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_combinations(params: dict) -> list[dict]:\n",
    "    '''\n",
    "    Creates and saves into the class instance a list with all the possible combinations of parameters \\\n",
    "    in the dictionary \"params\".\n",
    "\n",
    "    Parameters:\n",
    "    - params: dictionary with the parameters as associations <key:values> (with: key=parameter_name,\n",
    "    values=possible_values_list).\n",
    "\n",
    "    Returns:\n",
    "    A list of dictionaries such that each one represents a combination of parameters (1 combination = 1 dictionary).\n",
    "    '''\n",
    "    params_index_dict = {}\n",
    "    params_combinations = []\n",
    "    for key in params.keys():\n",
    "        params_index_dict[key] = 0 # current_index for that key\n",
    "    while sum([index+1 for _, index in params_index_dict.items()]) != sum(len(val_list) for _, val_list in params.items()):\n",
    "        params_i = {}\n",
    "        for key, i in params_index_dict.items():\n",
    "            params_i[key] = params[key][i]\n",
    "        params_combinations.append(params_i)\n",
    "        for key in params_index_dict.keys():\n",
    "            params_index_dict[key] += 1\n",
    "            if params_index_dict[key] < len(params[key]):\n",
    "                break\n",
    "            params_index_dict[key] = 0\n",
    "    params_i = {}\n",
    "    for key, i in params_index_dict.items():\n",
    "        params_i[key] = params[key][i]\n",
    "    params_combinations.append(params_i)\n",
    "    return params_combinations\n",
    "\n",
    "\n",
    "# Space of the hyperparameters\n",
    "params_space = {\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'epochs': [10, 30],\n",
    "    'architecture': [\n",
    "        {\n",
    "            'encoder_channels': [3,32,64], 'encoder_kernel_sizes': [3,3,3], 'encoder_strides': [1,1,1],\n",
    "            'decoder_channels': [64,32,1], 'decoder_kernel_sizes': [3,3,3], 'decoder_strides': [1,1,1],\n",
    "            'output_padding': 0\n",
    "        },\n",
    "        {\n",
    "            'encoder_channels': [3,32,64], 'encoder_kernel_sizes': [3,3,3], 'encoder_strides': [2,2,2],\n",
    "            'decoder_channels': [64,32,1], 'decoder_kernel_sizes': [2,2,2], 'decoder_strides': [2,2,2],\n",
    "            'output_padding': 0\n",
    "        },\n",
    "        {\n",
    "            'encoder_channels': [3,32,64], 'encoder_kernel_sizes': [5,5,5], 'encoder_strides': [2,2,2],\n",
    "            'decoder_channels': [64,32,1], 'decoder_kernel_sizes': [4,4,4], 'decoder_strides': [2,2,2],\n",
    "            'output_padding': 0\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([8, 1, 128, 128])) that is different to the input size (torch.Size([8, 1, 30, 30])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCELoss()\n\u001b[1;32m     24\u001b[0m \u001b[39m# Training phase\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m tr_losses, vl_losses \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m     26\u001b[0m     model,\n\u001b[1;32m     27\u001b[0m     tr_dataloader\u001b[39m=\u001b[39mtrain_loader,\n\u001b[1;32m     28\u001b[0m     vl_dataloader\u001b[39m=\u001b[39mval_loader,\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m     30\u001b[0m     criterion\u001b[39m=\u001b[39mcriterion,\n\u001b[1;32m     31\u001b[0m     device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     32\u001b[0m     epochs\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39m# Shows the Loss plots\u001b[39;00m\n\u001b[1;32m     36\u001b[0m show_plot(tr_losses, vl_losses)\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tr_dataloader, vl_dataloader, optimizer, criterion, device, epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mto(device)        \u001b[39m# Move masks' tensors on GPU or CPU\u001b[39;00m\n\u001b[1;32m     52\u001b[0m preds \u001b[39m=\u001b[39m model(imgs)             \u001b[39m# Compute the predictions\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m loss \u001b[39m=\u001b[39m criterion(preds, masks)  \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     54\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()           \u001b[39m# Reset the gradients\u001b[39;00m\n\u001b[1;32m     55\u001b[0m loss\u001b[39m.\u001b[39mbackward()                 \u001b[39m# Perform backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbinary_cross_entropy(\u001b[39minput\u001b[39m, target, weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3116\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3117\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3119\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3120\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3121\u001b[0m     )\n\u001b[1;32m   3123\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([8, 1, 128, 128])) that is different to the input size (torch.Size([8, 1, 30, 30])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def show_plot(tr_losses: list, vl_losses: list):\n",
    "    plt.plot(tr_losses, label='Training Loss')\n",
    "    plt.plot(vl_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss functions')\n",
    "    plt.show()\n",
    "\n",
    "# Iterates on each parameters combination obtained from the parameters space\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.70,0.15,0.15])\n",
    "for params in get_params_combinations(params_space):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Create the model\n",
    "    model = SegmentationCNN(**params['architecture'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # Training phase\n",
    "    tr_losses, vl_losses = train_model(\n",
    "        model,\n",
    "        tr_dataloader=train_loader,\n",
    "        vl_dataloader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        epochs=params['epochs']\n",
    "    )\n",
    "\n",
    "    # Shows the Loss plots\n",
    "    show_plot(tr_losses, vl_losses)\n",
    "\n",
    "    # Testing phase\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
